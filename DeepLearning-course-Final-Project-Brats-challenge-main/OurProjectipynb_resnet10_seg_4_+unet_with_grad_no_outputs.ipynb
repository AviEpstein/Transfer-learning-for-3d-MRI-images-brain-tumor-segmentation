{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": " OurProjectipynb resnet10 seg 4 +unet with grad_no_outputs",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TomerShimshi/DeepLearning-course-Final-Project-Brats-challenge/blob/main/OurProjectipynb_resnet10_seg_4_%2Bunet_with_grad_no_outputs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJEaFXjG4Ic-"
      },
      "source": [
        "import argparse\n",
        "!git clone https://github.com/Tencent/MedicalNet\n",
        "%load MedicalNet/setting.py\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcPt7pfEC7i-"
      },
      "source": [
        "load all relevent libs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5p6dd5BAC09"
      },
      "source": [
        "\n",
        "\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import torch, fastai, sys\n",
        "sys.path.insert(0, './exp')\n",
        "from sklearn.preprocessing import Binarizer\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "!pip install medpy\n",
        "from medpy import metric\n",
        "\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "import nibabel\n",
        "from scipy import ndimage\n",
        "import h5py\n",
        "from torchvision import transforms\n",
        "\n",
        "import torch, fastai, sys, os\n",
        "!pip install antspyx\n",
        "from fastai.vision import *\n",
        "#import ants\n",
        "import pathlib\n",
        "#from ants.core.ants_image import ANTsImage\n",
        "!pip install jupyterthemes\n",
        "from jupyterthemes import jtplot\n",
        "sys.path.insert(0, './exp')\n",
        "jtplot.style(theme='gruvboxd')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2buukobIpJ_"
      },
      "source": [
        "mount to drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtP8Sd2dIoSS"
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "path = Path('/content/drive/My Drive/Deep Learning Final Project/MICCAI_BraTS_2019_Data_Training')\n",
        "print(path)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ky1myKdF-Q4S"
      },
      "source": [
        "patch added the settings.py file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9eSnL_V-QZp"
      },
      "source": [
        "#@title argparser\n",
        "'''\n",
        "Configs for training & testing\n",
        "Written by Whalechen\n",
        "'''\n",
        "\n",
        "import argparse\n",
        "\n",
        "#parser = argparse.ArgumentParser()\n",
        "\n",
        "def parse_opts():\n",
        "    \n",
        "    parser = argparse.ArgumentParser()\n",
        "    \n",
        "    parser.add_argument(\n",
        "        '--data_root',\n",
        "        default='/content/drive/My Drive/Deep Learning Final Project/MICCAI_BraTS_2019_Data_Training',\n",
        "        type=str,\n",
        "        help='Root directory path of data')\n",
        "    parser.add_argument(\n",
        "        '--input_patch_size',\n",
        "        default='120, 136, 120',\n",
        "        type=str,\n",
        "        help='Root directory path of data')\n",
        "    parser.add_argument(\n",
        "        '--orig_patch_size',\n",
        "        default='120, 136, 120',\n",
        "        type=str,\n",
        "        help='list of HWD of input')\n",
        "    parser.add_argument(\n",
        "        '--img_list',\n",
        "        default='/content/drive/My Drive/Deep Learning Final Project/MICCAI_BraTS_2019_Data_Training/test.txt',\n",
        "        type=str,\n",
        "        help='Path for image list file')\n",
        "    parser.add_argument(\n",
        "        '--img_name',\n",
        "        default='Null',\n",
        "        type=str,\n",
        "        help='Path for image current img')\n",
        "    parser.add_argument('--iters', type=str,  default='11000,10000', help='checkpoint iteration(s)')\n",
        "    parser.add_argument(\n",
        "        '--n_seg_classes',\n",
        "        default=3,\n",
        "        type=int,\n",
        "        help=\"Number of segmentation classes\"\n",
        "    )\n",
        "    #print('finished 1 method')\n",
        "    parser.add_argument(\n",
        "        '--learning_rate',  # set to 0.001 when finetune\n",
        "        default=0.001,\n",
        "        type=float,\n",
        "        help=\n",
        "        'Initial learning rate (divided by 10 while training by lr scheduler)')\n",
        "    \n",
        "    parser.add_argument(\n",
        "        '--num_workers',\n",
        "        default=4,\n",
        "        type=int,\n",
        "        help='Number of jobs')\n",
        "    parser.add_argument(\n",
        "        '--batch_size', default=1, type=int, help='Batch Size')\n",
        "    parser.add_argument(\n",
        "        '--phase', default='train', type=str, help='Phase of train or test')\n",
        "    parser.add_argument(\n",
        "        '--save_intervals',\n",
        "        default=10,\n",
        "        type=int,\n",
        "        help='Interation for saving model')\n",
        "    parser.add_argument(\n",
        "        '--n_epochs',\n",
        "        default=200,\n",
        "        type=int,\n",
        "        help='Number of total epochs to run')\n",
        "    parser.add_argument(\n",
        "        '--input_D',\n",
        "    default=56,\n",
        "        type=int,\n",
        "        help='Input size of depth')\n",
        "    parser.add_argument(\n",
        "        '--input_H',\n",
        "        default=448,\n",
        "        type=int,\n",
        "        help='Input size of height')\n",
        "    parser.add_argument(\n",
        "        '--input_W',\n",
        "        default=448,\n",
        "        type=int,\n",
        "        help='Input size of width')\n",
        "    parser.add_argument(\n",
        "        '--resume_path',\n",
        "        default='',\n",
        "        type=str,\n",
        "        help=\n",
        "        'Path for resume model.'\n",
        "    )\n",
        "    \n",
        "    #print('finished 2 method')\n",
        "    parser.add_argument(\n",
        "        '--bce_weight',\n",
        "        default='[0., 3, 1, 1.75]',\n",
        "        type=list,\n",
        "        help=\n",
        "        'not sure, somthing for the loss func.'\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        '--new_layer_names',\n",
        "        #default=['upsample1', 'cmp_layer3', 'upsample2', 'cmp_layer2', 'upsample3', 'cmp_layer1', 'upsample4', 'cmp_conv1', 'conv_seg'],\n",
        "        default=['conv_seg'],\n",
        "        type=list,\n",
        "        help='New layer except for backbone')\n",
        "    parser.add_argument(\n",
        "        '--no_cuda', action='store_true', help='If true, cuda is not used.')\n",
        "    parser.set_defaults(no_cuda=False)\n",
        "    parser.add_argument(\n",
        "        '--gpu_id',\n",
        "        nargs='+',\n",
        "        type=int,              \n",
        "        help='Gpu id lists')\n",
        "    parser.add_argument(\n",
        "        '--model',\n",
        "        default='resnet',\n",
        "        type=str,\n",
        "        help='(resnet | preresnet | wideresnet | resnext | densenet | ')\n",
        "    parser.add_argument(\n",
        "        '--model_depth',\n",
        "        default=50,\n",
        "        type=int,\n",
        "        help='Depth of resnet (10 | 18 | 34 | 50 | 101)')\n",
        "    \n",
        "    parser.add_argument(\n",
        "        '--resnet_shortcut',\n",
        "        default='B',\n",
        "        type=str,\n",
        "        help='Shortcut type of resnet (A | B)')\n",
        "    \n",
        "    #print('finished 3 method')\n",
        "    \n",
        "    parser.add_argument(\n",
        "        '--manual_seed', default=1, type=int, help='Manually set random seed')\n",
        "    \n",
        "    #print('finished 4 method')\n",
        "\n",
        "    #!python3 parsing.py 5 \"arg2\"      \n",
        "\n",
        "    #parser.add_argument(\n",
        "     #   '--ci_test', action='store_true', help='If true, ci testing is used.')\n",
        "    parser.add_argument(\n",
        "        '--ci_test', action='store_true', help='If true, ci testing is used.')\n",
        "    args = parser.parse_args(args=[])\n",
        "    #args = parser.parse_args()\n",
        "    \n",
        "    #print('finished 5 method')\n",
        "    !python3 parsing.py 5\n",
        "    #for avi\n",
        "    #args.save_folder = \"/content/drive/My Drive/Deep Learning Final Project/trails/models/{}_{}\".format(args.model, args.model_depth)\n",
        "    #for shimshi\n",
        "    args.save_folder = \"/content/drive/My Drive/Deep Learning Final Project/trails/models/{}_{}\".format( args.model,  args.model_depth )\n",
        "    print('finished last method')\n",
        "    return args\n",
        "\n",
        "sets =parse_opts()  \n",
        "\n",
        "print('passed sets argparaser')\n",
        "\n",
        "sets.img_list = path/'test.txt' #'./toy_data/test_ci.txt' \n",
        "sets.n_epochs = 1\n",
        "sets.no_cuda = True\n",
        "sets.data_root = path # './toy_data'\n",
        "sets.pretrain_path =  path/'pretrain/resnet_50.pth'\n",
        "sets.num_workers = 0\n",
        "sets.model_depth = 50\n",
        "sets.resnet_shortcut = 'A'\n",
        "sets.input_D = 14\n",
        "sets.input_H = 28\n",
        "sets.input_W = 28\n",
        "\n",
        "print('finished sets')\n",
        "\n",
        "#BrainS18Dataset(sets.data_root, sets.img_list, sets)\n",
        "#BrainS18Dataset(path,loadtxt_str(path/'test.txt'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3TkZv8KIxAr"
      },
      "source": [
        "trying a new data loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQLAbnnWI0ot"
      },
      "source": [
        "\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "#----- this is the new brats map label\n",
        "\n",
        "def brats_map_label(labels, num_classes):\n",
        "    if type(labels) == torch.Tensor and torch.cuda.is_available():\n",
        "        labels_nhot = torch.zeros((num_classes,) + labels.shape, device='cuda')\n",
        "    else:\n",
        "        labels_nhot = np.zeros((num_classes,) + labels.shape, dtype=int)\n",
        "    #https://www.med.upenn.edu/cbica/brats2019/data.html and https://arxiv.org/pdf/1811.02629.pdf here is the mapping  page 10\n",
        "    labels_nhot[0, labels==0] = 1\n",
        "    labels_nhot[1,(labels==2)|(labels==3) |(labels==1)] = 1     #WT: Segmenting the whole tumor extent (Union of all labels)   # P(WT) = P(1)+P(2)+P(3)      \n",
        "    labels_nhot[2,labels==3] = 1    # P(ET) = P(3)\n",
        "    labels_nhot[3,(labels==1)|(labels==3)] = 1  #TC: Segmenting the gross tumor core outline # P(TC) = P(1)+P(3) \n",
        "    return labels_nhot\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def label_stats(root,img_list):\n",
        "    with open(img_list, 'r') as f:\n",
        "            img_dirs = [line.strip() for line in f]\n",
        "    \n",
        "    cls_total_counts = np.zeros(4, dtype=int)\n",
        "        \n",
        "    for img_dir in tqdm(img_dirs):\n",
        "        \n",
        "        img_path= str(root)+'/'+str(img_dir)+'/'+img_dir[3:] +  \".h5\"\n",
        "        \n",
        "        image_h5 = h5py.File(img_path)\n",
        "        labels = np.array(image_h5['label']).astype(int)\n",
        "        labels -= (labels == 4)\n",
        "        labels_nhot = brats_map_label(labels, 4)\n",
        "        cls_counts = labels_nhot.reshape(4, -1).sum(axis=1)\n",
        "        print('cls_counts={}'.format(cls_counts))\n",
        "        cls_total_counts += cls_counts\n",
        "\n",
        "        tempL = np.nonzero(labels_nhot[2] == 1)\n",
        "        # Find the boundary of non-zero labels\n",
        "        minx, maxx = np.min(tempL[0]), np.max(tempL[0])\n",
        "        miny, maxy = np.min(tempL[1]), np.max(tempL[1])\n",
        "        minz, maxz = np.min(tempL[2]), np.max(tempL[2])\n",
        "        H, W, D = maxx - minx, maxy - miny, maxz - minz\n",
        "        \n",
        "        print(\"%s => %s, %.2f%%\" %(labels.shape, (H, W, D), (100*H*W*D/labels.size)))\n",
        "                \n",
        "    print(cls_total_counts)\n",
        "    print(cls_total_counts / cls_total_counts.sum())\n",
        "    \n",
        "def covert_h5(root,img_list):\n",
        "    do_localization = False\n",
        "    \n",
        "    if 'val' in str(img_list.stem):\n",
        "        is_training = False\n",
        "    else:\n",
        "        print('training')\n",
        "        is_training = True\n",
        "     \n",
        "    \n",
        "    with open(img_list, 'r') as f:\n",
        "            img_dirs = [line.strip() for line in f]\n",
        "    \n",
        "    print('img_dirs = {}'.format(img_dirs))\n",
        "    modalities = [ 'flair', 't1ce', 't1', 't2' ]\n",
        "    if is_training:\n",
        "        print('added seg')\n",
        "        modalities.append('seg')\n",
        "        \n",
        "    for img_dir in tqdm(img_dirs):\n",
        "        image_mods = []\n",
        "        for mod in modalities:\n",
        "            \n",
        "            img_path= str(root)+'/'+str(img_dir)+'/'+img_dir[3:] + \"_%s.nii.gz\" %mod\n",
        "            image_obj = nibabel .load(img_path)\n",
        "            print('imge after load = {}'.format(image_obj))\n",
        "            image = image_obj.get_fdata()\n",
        "            image = image.astype(np.float32)\n",
        "            \n",
        "            image_mods.append(image)\n",
        "        \n",
        "        if is_training:\n",
        "            # image_mods contains five modalities, including 'seg'.\n",
        "            # Avoid putting 'seg' into 'image'.\n",
        "            image_mm = np.stack(image_mods[:-1], axis=0)\n",
        "            print('imge after np.stack = {}'.format(image_mm))\n",
        "            MOD, H, W, D = image_shape = image_mm.shape\n",
        "            # 'seg' => labels\n",
        "            labels = image_mods[-1].astype(np.uint8)\n",
        "            print(' labels ={}'.format(labels))\n",
        "        else:\n",
        "            # image_mods contains four modalities\n",
        "            image_mm = np.stack(image_mods, axis=0)\n",
        "            MOD, H, W, D = image_shape = image_mm.shape\n",
        "            # Save fake labels\n",
        "            labels = np.zeros_like(image_mods[0]).astype(np.uint8)\n",
        "            \n",
        "        if is_training and do_localization:\n",
        "            tempL = np.nonzero(labels)\n",
        "            # Find the boundary of non-zero labels\n",
        "            minx, maxx = np.min(tempL[1]), np.max(tempL[1])\n",
        "            miny, maxy = np.min(tempL[2]), np.max(tempL[2])\n",
        "            minz, maxz = np.min(tempL[3]), np.max(tempL[3])\n",
        "\n",
        "            # px, py, pz ensure the output image is at least of output_size\n",
        "            px = max(output_size[0] - (maxx - minx), 0) // 2\n",
        "            py = max(output_size[1] - (maxy - miny), 0) // 2\n",
        "            pz = max(output_size[2] - (maxz - minz), 0) // 2\n",
        "            # randint(10, 20) lets randomly-sized zero margins included in the output image\n",
        "            minx = max(minx - np.random.randint(10, 20) - px, 0)\n",
        "            maxx = min(maxx + np.random.randint(10, 20) + px, H)\n",
        "            miny = max(miny - np.random.randint(10, 20) - py, 0)\n",
        "            maxy = min(maxy + np.random.randint(10, 20) + py, W)\n",
        "            minz = max(minz - np.random.randint(5, 10) - pz, 0)\n",
        "            maxz = min(maxz + np.random.randint(5, 10) + pz, D)\n",
        "        # Do not crop black margins of validation images.\n",
        "        else:\n",
        "            tempL = np.nonzero(image_mm)\n",
        "            # Find the boundary of non-zero voxels\n",
        "            minx, maxx = np.min(tempL[1]), np.max(tempL[1])\n",
        "            miny, maxy = np.min(tempL[2]), np.max(tempL[2])\n",
        "            minz, maxz = np.min(tempL[3]), np.max(tempL[3])\n",
        "            \n",
        "        # On validation data, use image_crop to compute mean and std, and normalize image_mm.\n",
        "        image_crop = image_mm[:, minx:maxx, miny:maxy, minz:maxz]\n",
        "        if is_training:\n",
        "            image_mm = image_crop\n",
        "            labels = labels[minx:maxx, miny:maxy, minz:maxz]\n",
        "\n",
        "        nonzero_mask = (image_mm > 0)\n",
        "        for m in range(MOD):\n",
        "            image_mod      = image_mm[m, :, :, :]\n",
        "            image_mod_crop = image_crop[m, :, :, :]\n",
        "            nonzero_voxels = image_mod_crop[image_mod_crop > 0]\n",
        "            mean = nonzero_voxels.mean()\n",
        "            std  = nonzero_voxels.std()\n",
        "            image_mm[m, :, :, :] = (image_mod - mean) / std\n",
        "        \n",
        "        # Set voxels back to 0 if they are 0 before normalization.\n",
        "        image_mm *= nonzero_mask\n",
        "        print(\"\\n%s: %s => %s, %s\" %(img_dir, image_shape, image_mm.shape, labels.shape))\n",
        "\n",
        "        #h5_path = os.path.join(root, img_dir, img_dir + \".h5\")\n",
        "        h5_path = str(root)+'/'+str(img_dir)+'/'+img_dir[3:] + \".h5\"\n",
        "        f = h5py.File(h5_path, 'w')\n",
        "        f.create_dataset('image', data=image_mm, compression=\"gzip\")\n",
        "        f.create_dataset('label', data=labels,   compression=\"gzip\")\n",
        "        f.close()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    sets.img_list =   path/'train.txt'# path/'toy_data/test_ci.txt'\n",
        "    sets.n_epochs = 10\n",
        "    sets.no_cuda = True\n",
        "    sets.gpu_id = [0]\n",
        "    sets.data_root = path #/'toy_data'\n",
        "    sets.pretrain_path =  path/'pretrain/resnet_50.pth'\n",
        "    sets.num_workers = 0\n",
        "    sets.n_seg_classes = 3\n",
        "    sets.model_depth = 50\n",
        "    sets.resnet_shortcut = 'B'\n",
        "    sets.ci_test = False\n",
        "    sets.save_intervals=50\n",
        "    sets.n_epochs=15\n",
        "    #sets.resume_path= 'sets.resume_path=/content/drive/My Drive/Deep Learning Final Project/trails/models/resnet_50_epoch_9_batch_299.pth.tar'\n",
        "    sets.input_D = 14\n",
        "    sets.input_H = 28\n",
        "    sets.input_W = 28\n",
        "    convert = False\n",
        "    if convert:\n",
        "      covert_h5(path,sets.img_list)\n",
        "    \n",
        "    if sys.argv[1] == 'h5':\n",
        "        covert_h5(sys.argv[2])\n",
        "    elif sys.argv[1] == 'label':\n",
        "        label_stats(sys.argv[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Wk9bOlDYz8w"
      },
      "source": [
        "#the new dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RtHJQ6jY2xz"
      },
      "source": [
        "class BratsSet(Dataset):\n",
        "    \"\"\" Annual Brats challenges dataset \"\"\"\n",
        "    # binarize: whether to binarize mask (do whole-tumor segmentation)\n",
        "    # modality: if the image has multiple modalities, \n",
        "    # choose which modality to output (-1 to output all).\n",
        "    # If mode == 'train' and train_loc_prob > 0, then min_output_size is necessary.\n",
        "  \n",
        "    def __init__(self,sets, base_dir, img_list, sample_num=None, \n",
        "                 ds_weight=1.,\n",
        "                 xyz_permute=None, transform=None, \n",
        "                 chosen_modality=-1, binarize=False,\n",
        "                 train_loc_prob=0, min_output_size=None):\n",
        "        #super(BratsSet, self).__init__()\n",
        "        super(BratsSet, self).__init__()\n",
        "        self._base_dir = base_dir\n",
        "        self.split = sets.phase\n",
        "        self.mode = sets.phase\n",
        "        self.xyz_permute = xyz_permute\n",
        "        self.ds_weight = ds_weight\n",
        "        \n",
        "        self.transform = transform\n",
        "        self.chosen_modality = chosen_modality\n",
        "        self.binarize    = binarize\n",
        "        self.train_loc_prob = train_loc_prob\n",
        "        self.min_output_size = min_output_size\n",
        "        \n",
        "        if self.split == 'train':\n",
        "          trainlist_filepath = sets.img_list\n",
        "          with open(trainlist_filepath, 'r') as f:\n",
        "            self.train_image_list = f.readlines()\n",
        "        elif self.split == 'test':\n",
        "          testlist_filepath  = sets.img_list\n",
        "          with open(testlist_filepath,  'r') as f:\n",
        "            self.test_image_list = f.readlines()\n",
        "\n",
        "        \n",
        "\n",
        "        if self.split == 'train':\n",
        "            self.image_list = self.train_image_list\n",
        "        elif self.split == 'test':\n",
        "            self.image_list = self.test_image_list\n",
        "        elif self.split == 'all':\n",
        "            self.image_list = self.all_image_list\n",
        "\n",
        "        self.image_list = [ item.replace('\\n', '') for item in self.image_list ]\n",
        "        print('self.image_list ={}'.format(self.image_list))\n",
        "        if sample_num is not None:\n",
        "            self.image_list = self.image_list[:sample_num]\n",
        "                \n",
        "        self.num_modalities = 0\n",
        "        # Fetch image 0 to get num_modalities.\n",
        "        sample0 = self.__getitem__(0, do_transform=False)\n",
        "        if len(sample0['image'].shape) == 4:\n",
        "            self.num_modalities = sample0['image'].shape[0]\n",
        "        \n",
        "        print(\"'{}' {} samples, num_modalities: {}, chosen: {}\".format(self.split, \n",
        "                len(self.image_list), self.num_modalities, self.chosen_modality))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_list)\n",
        "        \n",
        "    def __getitem__(self, idx, do_transform=True):\n",
        "        image_name = self.image_list[idx]\n",
        "        \n",
        "        image_path = str(self._base_dir)+ '/'+str(image_name)+ '/'+image_name[3:] + \".h5\"\n",
        "        \n",
        "        try:\n",
        "           h5f = h5py.File(image_path, 'r')\n",
        "        except:\n",
        "          print('the pat {} is bad'.format(image_path))\n",
        "          return\n",
        "        image = h5f['image'][:]\n",
        "      \n",
        "        mask  = h5f['label'][:]\n",
        "\n",
        "      \n",
        "        if self.num_modalities > 0 and self.chosen_modality != -1:\n",
        "            image = image[self.chosen_modality, :, :, :]\n",
        "        if self.binarize:\n",
        "            print('binarize')\n",
        "            mask = (mask >= 1).astype(np.uint8)\n",
        "        else:\n",
        "            # Map 4 to 3, and keep 0,1,2 unchanged.\n",
        "            mask -= (mask == 4)\n",
        "\n",
        "        if self.mode == 'train' and self.train_loc_prob > 0 \\\n",
        "          and np.random.random() < self.train_loc_prob:\n",
        "            print('localize')\n",
        "            image, mask = localize(image, mask, self.min_output_size)\n",
        "\n",
        "        # xyz_permute by default is None.\n",
        "        if do_transform and self.xyz_permute is not None:\n",
        "            print('do_transform')\n",
        "            image = image.transpose(self.xyz_permute)\n",
        "            mask  = mask.transpose(self.xyz_permute)\n",
        "\n",
        "        if self.mode == 'train':\n",
        "          print('randome and randome noise')\n",
        "          k = np.random.randint(0, 4)\n",
        "\n",
        "          # Image has multiple modalities. First dimension is the modality.\n",
        "          # Only rotate along the x,y (0,1) plane.\n",
        "          if image.ndim == mask.ndim + 1:\n",
        "              image = np.rot90(image, k, axes=(1,2))\n",
        "          else:\n",
        "              image = np.rot90(image, k, axes=(0,1))\n",
        "\n",
        "          mask = np.rot90(mask, k, axes=(0,1))\n",
        "\n",
        "          # Flip along a random dimension. It doesn't change the shape of a tensor.\n",
        "          \n",
        "          axis = np.random.randint(0, 3)\n",
        "          if image.ndim == mask.ndim + 1:\n",
        "              image = np.flip(image, axis=axis+1).copy()\n",
        "          else:\n",
        "              image = np.flip(image, axis=axis).copy()\n",
        "\n",
        "          mask = np.flip(mask, axis=axis).copy()\n",
        "          ####### randome noise##############\n",
        "         \n",
        "        sample = { 'image': image, 'mask': mask }\n",
        "        \n",
        "        \n",
        "        \n",
        "       \n",
        "        if do_transform and self.transform:\n",
        "            sample = self.transform(sample)\n",
        "        \n",
        "        sample['image_path'] = image_name\n",
        "        sample['weight'] = self.ds_weight\n",
        "       \n",
        "        return sample\n",
        "\n",
        "    def create_file_list(self, train_test_split):\n",
        "        img_dirs = [ d for d in listdir(self._base_dir) if isdir(join(self._base_dir, d)) ]\n",
        "\n",
        "        # Randomize the file list, then split. Not to use the official testing set \n",
        "        # since we don't have ground truth masks for this.\n",
        "        num_files = len(img_dirs)\n",
        "        idxList = np.arange(num_files)  # List of file indices\n",
        "        self.imgFiles = {}\n",
        "        for idx in idxList:\n",
        "            self.imgFiles[idx] = join(img_dirs[idx], img_dirs[idx] + \".h5\")\n",
        "\n",
        "        with open( join(self._base_dir, 'all.list'), \"w\" ) as allFile:\n",
        "            for img_idx in idxList:\n",
        "                allFile.write(\"%s\\n\" %self.imgFiles[img_idx])\n",
        "        allFile.close()\n",
        "                \n",
        "        idxList = np.random.permutation(idxList)  # Randomize list\n",
        "        train_len = int(np.floor(num_files * train_test_split)) # Number of training files\n",
        "        train_indices = idxList[0:train_len]  # List of training indices\n",
        "        test_indices  = idxList[train_len:]  # List of testing indices\n",
        "\n",
        "        with open( join(self._base_dir, 'train.list'), \"w\" ) as trainFile:\n",
        "            for img_idx in sorted(train_indices):\n",
        "                trainFile.write(\"%s\\n\" %self.imgFiles[img_idx])\n",
        "        trainFile.close()\n",
        "        \n",
        "        with open( join(self._base_dir, 'test.list'),  \"w\" ) as testFile:\n",
        "            for img_idx in sorted(test_indices):\n",
        "                testFile.write(\"%s\\n\"  %self.imgFiles[img_idx])\n",
        "        testFile.close()\n",
        "        \n",
        "        print(\"%d files are split to %d training, %d test\" %(num_files, train_len, len(test_indices)))\n",
        "            \n",
        "class CenterCrop(object):\n",
        "    def __init__(self, output_size):\n",
        "        self.output_size = output_size\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, mask = sample['image'], sample['mask']\n",
        "        \n",
        "        # pad the sample if necessary\n",
        "        if mask.shape[0] <= self.output_size[0] or mask.shape[1] <= self.output_size[1] or mask.shape[2] <= \\\n",
        "                self.output_size[2]:\n",
        "            ph = max((self.output_size[0] - mask.shape[0]) // 2 + 3, 0)\n",
        "            pw = max((self.output_size[1] - mask.shape[1]) // 2 + 3, 0)\n",
        "            pd = max((self.output_size[2] - mask.shape[2]) // 2 + 3, 0)\n",
        "            mask_padding = [(ph, ph), (pw, pw), (pd, pd)]\n",
        "            print('padddd')\n",
        "            image_padding = mask_padding\n",
        "            if len(image.shape) == 4:\n",
        "                image_padding = [(0, 0)] + image_padding\n",
        "\n",
        "            image = np.pad(image, image_padding, mode='constant', constant_values=0)\n",
        "            mask  = np.pad(mask,  mask_padding,  mode='constant', constant_values=0)\n",
        "\n",
        "        (h, w, d) = image.shape[:3]\n",
        "\n",
        "        h1 = int(round((h - self.output_size[0]) / 2.))\n",
        "        w1 = int(round((w - self.output_size[1]) / 2.))\n",
        "        d1 = int(round((d - self.output_size[2]) / 2.))\n",
        "\n",
        "        mask  = mask[h1:h1  + self.output_size[0], w1:w1 + self.output_size[1], d1:d1 + self.output_size[2]]\n",
        "        if len(image.shape) == 4:\n",
        "            image = image[:, h1:h1 + self.output_size[0], w1:w1 + self.output_size[1], d1:d1 + self.output_size[2]]\n",
        "        else:\n",
        "            image = image[h1:h1 + self.output_size[0], w1:w1 + self.output_size[1], d1:d1 + self.output_size[2]]\n",
        "                \n",
        "        return {'image': image, 'mask': mask}\n",
        "\n",
        "class RandomCrop(object):\n",
        "    \"\"\"\n",
        "    Crop randomly the image in a sample\n",
        "    Args:\n",
        "    output_size (int): Desired output size\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_size):\n",
        "        self.output_size = output_size\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, mask = sample['image'], sample['mask']\n",
        "        orig_mask_shape = list(mask.shape)\n",
        "        \n",
        "        # pad the sample if necessary\n",
        "        if mask.shape[0] <= self.output_size[0] or mask.shape[1] <= self.output_size[1] or mask.shape[2] <= \\\n",
        "                self.output_size[2]:\n",
        "            ph = max((self.output_size[0] - mask.shape[0]) // 2 + 3, 0)\n",
        "            pw = max((self.output_size[1] - mask.shape[1]) // 2 + 3, 0)\n",
        "            pd = max((self.output_size[2] - mask.shape[2]) // 2 + 3, 0)\n",
        "            mask_padding = [(ph, ph), (pw, pw), (pd, pd)]\n",
        "            image_padding = mask_padding\n",
        "            # the modality dimension.\n",
        "            if len(image.shape) == 4:\n",
        "                image_padding = [(0, 0)] + image_padding\n",
        "            \n",
        "            try:\n",
        "                image = np.pad(image, image_padding, mode='constant', constant_values=0)\n",
        "                mask  = np.pad(mask,  mask_padding,  mode='constant', constant_values=0)\n",
        "            except:\n",
        "                pdb.set_trace()\n",
        "        else:\n",
        "            image_padding = 0\n",
        "            \n",
        "        \n",
        "        (h, w, d) = image.shape[-3:]\n",
        "        # if np.random.uniform() > 0.33:\n",
        "        #     w1 = np.random.randint((w - self.output_size[0])//4, 3*(w - self.output_size[0])//4)\n",
        "        #     h1 = np.random.randint((h - self.output_size[1])//4, 3*(h - self.output_size[1])//4)\n",
        "        # else:\n",
        "        h1 = np.random.randint(0, h - self.output_size[0])\n",
        "        w1 = np.random.randint(0, w - self.output_size[1])\n",
        "        d1 = np.random.randint(0, d - self.output_size[2])\n",
        "        # print(h1, h1 + self.output_size[0], w1, w1 + self.output_size[1], d1, d1 + self.output_size[2])\n",
        "            \n",
        "        mask  = mask[h1:h1 + self.output_size[0], w1:w1 + self.output_size[1], d1:d1 + self.output_size[2]]\n",
        "        # image: (118, 118, 102). mask: (118, 118, 102) => (112, 112, 96)\n",
        "        if image.ndim == 4:\n",
        "            # For a 4D image, the first dimension is modality.\n",
        "            image = image[:, h1:h1 + self.output_size[0], w1:w1 + self.output_size[1], d1:d1 + self.output_size[2]]\n",
        "        else:\n",
        "            image = image[h1:h1 + self.output_size[0], w1:w1 + self.output_size[1], d1:d1 + self.output_size[2]]\n",
        "\n",
        "        return {'image': image, 'mask': mask}\n",
        "\n",
        "\n",
        "class RandomRotFlip(object):\n",
        "    \"\"\"\n",
        "    Crop randomly flip the dataset in a sample\n",
        "    Args:\n",
        "    output_size (int): Desired output size\n",
        "    \"\"\"\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, mask = sample['image'], sample['mask']\n",
        "        assert (image.ndim == mask.ndim) or (image.ndim == mask.ndim + 1)\n",
        "        # Rotate by k*90. k is not the axis.\n",
        "        k = np.random.randint(0, 4)\n",
        "\n",
        "        # Image has multiple modalities. First dimension is the modality.\n",
        "        # Only rotate along the x,y (0,1) plane.\n",
        "        if image.ndim == mask.ndim + 1:\n",
        "            image = np.rot90(image, k, axes=(1,2))\n",
        "        else:\n",
        "            image = np.rot90(image, k, axes=(0,1))\n",
        "            \n",
        "        mask = np.rot90(mask, k, axes=(0,1))\n",
        "        \n",
        "        # Flip along a random dimension. It doesn't change the shape of a tensor.\n",
        "        axis = np.random.randint(0, 3)\n",
        "        if image.ndim == mask.ndim + 1:\n",
        "            image = np.flip(image, axis=axis+1).copy()\n",
        "        else:\n",
        "            image = np.flip(image, axis=axis).copy()\n",
        "            \n",
        "        mask = np.flip(mask, axis=axis).copy()\n",
        "\n",
        "        return { 'image': image, 'mask': mask }\n",
        "\n",
        "\n",
        "class RandomNoise(object):\n",
        "    def __init__(self, mu=0, sigma=0.1, nonzero_only=True):\n",
        "        self.mu = mu\n",
        "        self.sigma = sigma\n",
        "        self.nonzero_only = nonzero_only\n",
        "        \n",
        "    def __call__(self, sample):\n",
        "        image, mask = sample['image'], sample['mask']\n",
        "        noise = np.clip(self.sigma * np.random.randn(*image.shape), -2*self.sigma, 2*self.sigma)\n",
        "        noise = noise + self.mu\n",
        "        if self.nonzero_only:\n",
        "            # Only add noise to non-zero voxels\n",
        "            image = image + noise * (image != 0)\n",
        "        else:\n",
        "            image = image + noise\n",
        "            \n",
        "        return {'image': image, 'mask': mask}\n",
        "\n",
        "class ToTensor(object):\n",
        "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image = sample['image']\n",
        "        mask  = sample['mask']\n",
        "        if image.ndim == 3:\n",
        "            image = image.reshape((1,) + image.shape)\n",
        "        return { 'image': torch.from_numpy(image), \n",
        "                 'mask':  torch.from_numpy(mask) }\n",
        "\n",
        "# mask should be one-hot. shape: Batch, Class, H, W, D\n",
        "def RandomResizedCrop(volume, mask, out_size, crop_percents, isotropic=True):\n",
        "    H, W, D = volume.shape[-3:]\n",
        "    min_crop, max_crop = crop_percents  # -0.1, 0.1\n",
        "    min_scale = 1 + min_crop            # 0.9\n",
        "    max_scale = 1 + max_crop            # 1.1\n",
        "    \n",
        "    if not isotropic:\n",
        "        scale_H = torch.rand(1) * (max_scale - min_scale) + min_scale\n",
        "        scale_W = torch.rand(1) * (max_scale - min_scale) + min_scale\n",
        "        scale_D = torch.rand(1) * (max_scale - min_scale) + min_scale\n",
        "    else:\n",
        "        # scale_H is uniform between [min_scale, max_scale]\n",
        "        scale_H = torch.rand(1) * (max_scale - min_scale) + min_scale\n",
        "        scale_W = scale_D = scale_H\n",
        "\n",
        "    H2 = int(H * scale_H)\n",
        "    W2 = int(W * scale_W)\n",
        "    D2 = int(D * scale_D)\n",
        "    \n",
        "    # volume: [4, 1, 112, 112, 96]. mask: [4, 4, 112, 112, 96]\n",
        "    volume2 = F.interpolate(volume, size=(H2, W2, D2), mode='trilinear', align_corners=False)\n",
        "    mask2   = F.interpolate(mask,   size=(H2, W2, D2), mode='trilinear', align_corners=False)\n",
        "    \n",
        "    # out_size: (112, 112, 96)\n",
        "    H_out, W_out, D_out = out_size\n",
        "    if H2 < H_out or W2 < W_out or D2 < D_out:\n",
        "        pad_h  = max(H_out - H2, 0)\n",
        "        pad_h1 = pad_h // 2\n",
        "        pad_h2 = pad_h - pad_h1\n",
        "        pad_w  = max(W_out - W2, 0)\n",
        "        pad_w1 = pad_w // 2\n",
        "        pad_w2 = pad_w - pad_w1\n",
        "        pad_d  = max(D_out - D2, 0)\n",
        "        pad_d1 = pad_d // 2\n",
        "        pad_d2 = pad_d - pad_d1\n",
        "        \n",
        "        pads = (pad_d1, pad_d2, pad_w1, pad_w2, pad_h1, pad_h2)\n",
        "        \n",
        "        volume2 = F.pad(volume2, pads, \"constant\", 0)\n",
        "        mask2   = F.pad(mask2,   pads, \"constant\", 0)\n",
        "    \n",
        "    H2, W2, D2 = volume2.shape[2:] \n",
        "    h_start = torch.randint(H2 - H_out + 1, (1,))\n",
        "    h_end   = h_start + H_out\n",
        "    w_start = torch.randint(W2 - W_out + 1, (1,))\n",
        "    w_end   = w_start + W_out\n",
        "    d_start = torch.randint(D2 - D_out + 1, (1,))\n",
        "    d_end   = d_start + D_out\n",
        "        \n",
        "    volume3 = volume2[:, :, h_start:h_end, w_start:w_end, d_start:d_end].clone()\n",
        "    mask3   = mask2[:,   :, h_start:h_end, w_start:w_end, d_start:d_end].clone()\n",
        "    # If converting the continuous mask values (produced by interpolation) to one-hot, performance drops. So keep the cont\n",
        "    # mask3   = (mask3 >= 0.5).type(mask.dtype)\n",
        "    \n",
        "    return volume3, mask3\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "      # settting\n",
        "      sets =parse_opts() \n",
        "      sets.img_list =   path/'train.txt'# path/'toy_data/test_ci.txt'\n",
        "      sets.n_epochs = 10\n",
        "      sets.no_cuda = False\n",
        "      sets.gpu_id = [0]\n",
        "      sets.data_root = path #/'toy_data'\n",
        "      sets.pretrain_path =  path/'pretrain/resnet_50.pth'\n",
        "      sets.num_workers = 0\n",
        "      sets.n_seg_classes = 3\n",
        "      sets.model_depth = 50\n",
        "      sets.resnet_shortcut = 'B'\n",
        "      sets.phase = 'train'\n",
        "      sets.ci_test = False\n",
        "      sets.save_intervals=50\n",
        "      sets.n_epochs=15\n",
        "      sets.input_D = 14\n",
        "      sets.input_H = 28\n",
        "      sets.input_W = 28\n",
        "      sets.save_folder = \"/content/drive/My Drive/Deep Learning Final Project/trails/models/{}_{}\".format( sets.model,  sets.model_depth )\n",
        "\n",
        "\n",
        "      training_dataset = BratsSet(sets,sets.data_root, sets.img_list )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "di66eH9NADS7"
      },
      "source": [
        "the dataloader code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aL6sRV9r4Bdd"
      },
      "source": [
        "trying to load it from the drive\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lut8pI55MgzS"
      },
      "source": [
        "***vizualizing the loaded data:***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "WzedC5P1MrbD"
      },
      "source": [
        "#@title vizualizing the loaded data\n",
        "from skimage.util import montage as montage2d\n",
        "from torch.utils.data import DataLoader\n",
        "# getting data\n",
        "data_loader = DataLoader(training_dataset, batch_size=sets.batch_size, shuffle=True, num_workers=sets.num_workers)\n",
        "\n",
        "dataset_iter = iter(data_loader)\n",
        "batch_data = next(dataset_iter)\n",
        "\n",
        "volumes, mask = batch_data['image'], batch_data['mask']\n",
        "print('vizualiztion demo: volumes.shape =  {}'.format(volumes.shape))\n",
        "print('vizualiztion demo: mask.shape =  {}'.format(mask.shape))\n",
        "\n",
        "display_img_demo = volumes[0,0] #need to cheek if its T1,flair...\n",
        "display_label_demo = mask[0] #this is target for all\n",
        "\n",
        "img_vizualization_demo_path = Path('/content/drive/My Drive/Deep Learning Final Project/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_2013_2_1/BraTS19_2013_2_1_t1.nii.gz')\n",
        "label_vizualization_demo_path = Path('/content/drive/My Drive/Deep Learning Final Project/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_2013_2_1/BraTS19_2013_2_1_seg.nii.gz')\n",
        "\n",
        "img_vizualization_demo_without_data_procesing = nibabel.load(img_vizualization_demo_path)\n",
        "label_vizualization_demo_without_data_procesing = nibabel.load(label_vizualization_demo_path)\n",
        "\n",
        "img_vizualization_demo_without_data_procesing = img_vizualization_demo_without_data_procesing.get_fdata()\n",
        "label_vizualization_demo_without_data_procesing = label_vizualization_demo_without_data_procesing.get_fdata()\n",
        "\n",
        "\n",
        "def vizualize_a_single_img_and_its_lable(display_img, display_label, montage_display = False):\n",
        "    #exceptts img opened in nibibil that have gone throw get fdata\n",
        "\n",
        "    print('vizualiztion: display_img.shape  =  {}'.format(display_img.shape))\n",
        "    print('vizualiztion: display_label.shape  =  {}'.format(display_label.shape))\n",
        "\n",
        "    #picking a particular slice of display img here we used display_img.shape[2]//2 (intger divsion)\n",
        "    #needs to be changed depending on our implamataion of T1,flair.. or all of them combined as 4 channels\n",
        "    #displaying one image (T1 or flair or..) and its totall lable of a slice:\n",
        "    fig, (ax1, ax2) = plt.subplots(1,2, figsize = (12, 6))\n",
        "    ax1.imshow(display_img[:, :,display_img.shape[2]//2].T,cmap ='bone') #needs to be changed depending on our implamataion of T1,flair.. or all of them combined as 4 channels\n",
        "    ax1.set_title('Image')\n",
        "    ax2.imshow(display_label[:, :,display_label.shape[2]//2].T,cmap ='bone')\n",
        "    ax2.set_title('label')\n",
        "    \n",
        "    if (montage_display == True):\n",
        "      #displaying the 3d display_img as a montage:\n",
        "      fig, ax1 = plt.subplots(1, 1, figsize = (12, 6))\n",
        "      ax1.imshow(montage2d(display_img), cmap ='bone')\n",
        "      \n",
        "\n",
        "      #displaying the 3d display_label as a montage:\n",
        "      fig, ax1 = plt.subplots(1,1, figsize = (12, 6))\n",
        "      ax1.imshow(montage2d(display_label), cmap ='bone')\n",
        "      #fig.savefig('') #saving the label\n",
        "\n",
        "vizualize_a_single_img_and_its_lable(display_img_demo,display_label_demo,True)\n",
        "vizualize_a_single_img_and_its_lable(img_vizualization_demo_without_data_procesing,label_vizualization_demo_without_data_procesing,True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zG3mhsX1_6l"
      },
      "source": [
        "**add the resnet models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yd0m3Hlq49qJ",
        "cellView": "code"
      },
      "source": [
        "#@title resnet models\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import math\n",
        "from functools import partial\n",
        "\n",
        "__all__ = [\n",
        "    'ResNet', 'resnet10', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
        "    'resnet152', 'resnet200'\n",
        "]\n",
        "# https://www.youtube.com/watch?v=DkNIBBBvcPs&list=PLhhyoLH6IjfxeoooqP9rhU3HJIAVAJ3Vz&index=19 resnet video\n",
        "\n",
        "def conv3x3x3(in_planes, out_planes, stride=1, dilation=1):\n",
        "    # 3x3x3 convolution with padding\n",
        "    return nn.Conv3d(\n",
        "        in_planes,\n",
        "        out_planes,\n",
        "        kernel_size=3,\n",
        "        dilation=dilation,\n",
        "        stride=stride,\n",
        "        padding=dilation,\n",
        "        bias=False)\n",
        "\n",
        "\n",
        "def downsample_basic_block(x, planes, stride, no_cuda=False):\n",
        "    out = F.avg_pool3d(x, kernel_size=1, stride=stride)\n",
        "    zero_pads = torch.Tensor(\n",
        "        out.size(0), planes - out.size(1), out.size(2), out.size(3),\n",
        "        out.size(4)).zero_()\n",
        "    if not no_cuda:\n",
        "        if isinstance(out.data, torch.cuda.FloatTensor):\n",
        "            zero_pads = zero_pads.cuda()\n",
        "\n",
        "    out = Variable(torch.cat([out.data, zero_pads], dim=1))\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, dilation=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3x3(inplanes, planes, stride=stride, dilation=dilation)\n",
        "        self.bn1 = nn.BatchNorm3d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3x3(planes, planes, dilation=dilation)\n",
        "        self.bn2 = nn.BatchNorm3d(planes)\n",
        "        self.downsample = downsample #identity_downsample in resnet video\n",
        "        self.stride = stride\n",
        "        self.dilation = dilation\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x #identity in resnet video\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x) #idnetity_downsample if we need to change the shape\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, dilation=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv3d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm3d(planes)\n",
        "        self.conv2 = nn.Conv3d(\n",
        "            planes, planes, kernel_size=3, stride=stride, dilation=dilation, padding=dilation, bias=False)\n",
        "        self.bn2 = nn.BatchNorm3d(planes)\n",
        "        self.conv3 = nn.Conv3d(planes, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm3d(planes * 4)\n",
        "        \n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "        self.dilation = dilation\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:# add identity\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 block,    #the block\n",
        "                 layers,      #list of how many times we want to use the block\n",
        "                 sample_input_D,\n",
        "                 sample_input_H,\n",
        "                 sample_input_W,\n",
        "                 num_seg_classes,\n",
        "                 shortcut_type='B',\n",
        "                 no_cuda = False):\n",
        "        self.inplanes = 64\n",
        "        self.no_cuda = no_cuda\n",
        "        super(ResNet, self).__init__()\n",
        "        self.conv1 = nn.Conv3d(\n",
        "            1,\n",
        "            64,\n",
        "            kernel_size=7,\n",
        "            stride=(2, 2, 2),\n",
        "            padding=(3, 3, 3),\n",
        "            bias=False) #intial layers (we havint done any resnet layers yet)\n",
        "            \n",
        "        self.bn1 = nn.BatchNorm3d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool3d(kernel_size=(3, 3, 3), stride=2, padding=1)\n",
        "\n",
        "        # from here we do resnet layers:\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0], shortcut_type) #make layer defined a few lines down / 256 at end\n",
        "        self.layer2 = self._make_layer(\n",
        "            block, 128, layers[1], shortcut_type, stride=2) #512 at end\n",
        "        self.layer3 = self._make_layer(\n",
        "            block, 256, layers[2], shortcut_type, stride=1, dilation=2) #1024\n",
        "        self.layer4 = self._make_layer(\n",
        "            block, 512, layers[3], shortcut_type, stride=1, dilation=4) #2048\n",
        "\n",
        "        print('num_seg_classes= {}'.format(num_seg_classes))\n",
        "\n",
        "        self.conv_seg = nn.Sequential( # this is the specific implamantation for our segmentation problam\n",
        "                                        nn.ConvTranspose3d(\n",
        "                                        512 * block.expansion,\n",
        "                                        32,\n",
        "                                        2,\n",
        "                                        stride=2\n",
        "                                        ),\n",
        "                                        nn.BatchNorm3d(32),\n",
        "                                        nn.ReLU(inplace=True),\n",
        "                                        nn.Conv3d(\n",
        "                                        32,\n",
        "                                        32,\n",
        "                                        kernel_size=3,\n",
        "                                        stride=(1, 1, 1),\n",
        "                                        padding=(1, 1, 1),\n",
        "                                        bias=False), \n",
        "                                        nn.BatchNorm3d(32),\n",
        "                                        nn.ReLU(inplace=True),\n",
        "                                        nn.Conv3d(\n",
        "                                        32,\n",
        "                                        num_seg_classes,\n",
        "                                        kernel_size=1,\n",
        "                                        stride=(1, 1, 1),\n",
        "                                        bias=False) )\n",
        "                                        \n",
        "        \n",
        "                                      \n",
        "                                       \n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv3d):\n",
        "                m.weight = nn.init.kaiming_normal(m.weight, mode='fan_out')\n",
        "            elif isinstance(m, nn.BatchNorm3d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, shortcut_type, stride=1, dilation=1):# blocks = num of times we use the block, plans=out channels.\n",
        "        downsample = None #identity downsample\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion: # when do we do identity downsample ie when conv layer change the identity (the input size changed)\n",
        "            if shortcut_type == 'A':\n",
        "                downsample = partial(\n",
        "                    downsample_basic_block,\n",
        "                    planes=planes * block.expansion,\n",
        "                    stride=stride,\n",
        "                    no_cuda=self.no_cuda)\n",
        "            else:\n",
        "                downsample = nn.Sequential(\n",
        "                    nn.Conv3d(\n",
        "                        self.inplanes,\n",
        "                        planes * block.expansion,\n",
        "                        kernel_size=1,\n",
        "                        stride=stride,\n",
        "                        bias=False), nn.BatchNorm3d(planes * block.expansion))# here we implemented the identity down sample\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride=stride, dilation=dilation, downsample=downsample))#this layer changes the number of channels firs layer changes to 256 at the end of the block (here changes stride or/and num of channels)\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, dilation=dilation)) #256->64, 64*4(256 again) so no identity down sample in this case\n",
        "\n",
        "        return nn.Sequential(*layers) # unpack the list\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.conv_seg(x)\n",
        "        \n",
        "\n",
        "        return x\n",
        "\n",
        "def resnet10(**kwargs):\n",
        "    \"\"\"Constructs a ResNet-18 model.\n",
        "    \"\"\"\n",
        "    model = ResNet(BasicBlock, [1, 1, 1, 1], **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet18(**kwargs):\n",
        "    \"\"\"Constructs a ResNet-18 model.\n",
        "    \"\"\"\n",
        "    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet34(**kwargs):\n",
        "    \"\"\"Constructs a ResNet-34 model.\n",
        "    \"\"\"\n",
        "    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet50(**kwargs):\n",
        "    \"\"\"Constructs a ResNet-50 model.\n",
        "    \"\"\"\n",
        "    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet101(**kwargs):\n",
        "    \"\"\"Constructs a ResNet-101 model.\n",
        "    \"\"\"\n",
        "    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet152(**kwargs):\n",
        "    \"\"\"Constructs a ResNet-101 model.\n",
        "    \"\"\"\n",
        "    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet200(**kwargs):\n",
        "    \"\"\"Constructs a ResNet-101 model.\n",
        "    \"\"\"\n",
        "    model = ResNet(Bottleneck, [3, 24, 36, 3], **kwargs)\n",
        "    return model\n",
        "\n",
        "# def test():\n",
        "#   net = resnet50() try all of the resnets\n",
        "#   x=torch.randn(2,image_dim needs to be cheeked channels and 3d dim)\n",
        "#   y =net(x).to('cuda')\n",
        "#   print (y.shape)\n",
        "#test()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQowvx1K4_Xi"
      },
      "source": [
        "**add the model creating code**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHoNL9G3vgcU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6AuLts32IM2"
      },
      "source": [
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "\n",
        "def generate_model(opt):\n",
        "    assert opt.model in [\n",
        "        'resnet'\n",
        "    ]\n",
        "\n",
        "    if opt.model == 'resnet':\n",
        "        assert opt.model_depth in [10, 18, 34, 50, 101, 152, 200]\n",
        "        \n",
        "        if opt.model_depth == 10:\n",
        "            model = resnet10(\n",
        "                sample_input_W=opt.input_W,\n",
        "                sample_input_H=opt.input_H,\n",
        "                sample_input_D=opt.input_D,\n",
        "                shortcut_type=opt.resnet_shortcut,\n",
        "                no_cuda=opt.no_cuda,\n",
        "                num_seg_classes=opt.n_seg_classes)\n",
        "        elif opt.model_depth == 18:\n",
        "            model = resnet18(\n",
        "                sample_input_W=opt.input_W,\n",
        "                sample_input_H=opt.input_H,\n",
        "                sample_input_D=opt.input_D,\n",
        "                shortcut_type=opt.resnet_shortcut,\n",
        "                no_cuda=opt.no_cuda,\n",
        "                num_seg_classes=opt.n_seg_classes)\n",
        "        elif opt.model_depth == 34:\n",
        "            model = resnet34(\n",
        "                sample_input_W=opt.input_W,\n",
        "                sample_input_H=opt.input_H,\n",
        "                sample_input_D=opt.input_D,\n",
        "                shortcut_type=opt.resnet_shortcut,\n",
        "                no_cuda=opt.no_cuda,\n",
        "                num_seg_classes=opt.n_seg_classes)\n",
        "        elif opt.model_depth == 50:\n",
        "            model = resnet50(\n",
        "                sample_input_W=opt.input_W,\n",
        "                sample_input_H=opt.input_H,\n",
        "                sample_input_D=opt.input_D,\n",
        "                shortcut_type=opt.resnet_shortcut,\n",
        "                no_cuda=opt.no_cuda,\n",
        "                num_seg_classes=opt.n_seg_classes)\n",
        "        elif opt.model_depth == 101:\n",
        "            model = resnet.resnet101(\n",
        "                sample_input_W=opt.input_W,\n",
        "                sample_input_H=opt.input_H,\n",
        "                sample_input_D=opt.input_D,\n",
        "                shortcut_type=opt.resnet_shortcut,\n",
        "                no_cuda=opt.no_cuda,\n",
        "                num_seg_classes=opt.n_seg_classes)\n",
        "        elif opt.model_depth == 152:\n",
        "            model = resnet.resnet152(\n",
        "                sample_input_W=opt.input_W,\n",
        "                sample_input_H=opt.input_H,\n",
        "                sample_input_D=opt.input_D,\n",
        "                shortcut_type=opt.resnet_shortcut,\n",
        "                no_cuda=opt.no_cuda,\n",
        "                num_seg_classes=opt.n_seg_classes)\n",
        "        elif opt.model_depth == 200:\n",
        "            model = resnet.resnet200(\n",
        "                sample_input_W=opt.input_W,\n",
        "                sample_input_H=opt.input_H,\n",
        "                sample_input_D=opt.input_D,\n",
        "                shortcut_type=opt.resnet_shortcut,\n",
        "                no_cuda=opt.no_cuda,\n",
        "                num_seg_classes=opt.n_seg_classes)\n",
        "    \n",
        "    if not opt.no_cuda:\n",
        "        if len(opt.gpu_id) > 1:\n",
        "            model = model.cuda() \n",
        "            model = nn.DataParallel(model, device_ids=opt.gpu_id)\n",
        "            net_dict = model.state_dict() \n",
        "        else:\n",
        "            import os\n",
        "            \n",
        "            print('running with cuda')\n",
        "            os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(opt.gpu_id[0])\n",
        "            model = model.cuda() \n",
        "            model = nn.DataParallel(model, device_ids=None)\n",
        "            net_dict = model.state_dict()\n",
        "    else:\n",
        "       \n",
        "        net_dict = model.state_dict()\n",
        "    \n",
        "    # load pretrain\n",
        "    #if opt.phase != 'test' and opt.pretrain_path:\n",
        "    if  opt.pretrain_path:\n",
        "        print ('loading pretrained model {}'.format(opt.pretrain_path))\n",
        "        print('test otzi')\n",
        "        if not opt.no_cuda:\n",
        "          pretrain = torch.load(opt.pretrain_path)\n",
        "        else:\n",
        "          pretrain = torch.load(opt.pretrain_path, map_location='cpu')\n",
        "        pretrain_dict = {k: v for k, v in pretrain['state_dict'].items() if k in net_dict.keys()}\n",
        "         \n",
        "        net_dict.update(pretrain_dict)\n",
        "        model.load_state_dict(net_dict)\n",
        "\n",
        "        new_parameters = [] \n",
        "        for pname, p in model.named_parameters():\n",
        "            for layer_name in opt.new_layer_names:\n",
        "                if pname.find(layer_name) >= 0:\n",
        "                    new_parameters.append(p)\n",
        "                    break\n",
        "        \n",
        "        new_parameters_id = list(map(id, new_parameters))\n",
        "        base_parameters = list(filter(lambda p: id(p) not in new_parameters_id, model.parameters()))\n",
        "        parameters = {'base_parameters': base_parameters, \n",
        "                      'new_parameters': new_parameters}\n",
        "        '''\n",
        "        for param in parameters:\n",
        "           param.requires_grad = False  ### moved here by avi insteed of in training methode\n",
        "        '''\n",
        "        print('returned parameters')\n",
        "        return model, parameters\n",
        "\n",
        "    print('returned model.parameter')\n",
        "    return model, model.parameters()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8ZMuPywAmdk"
      },
      "source": [
        "adding a utils func"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NahemkmqArR1"
      },
      "source": [
        "\n",
        "class SmoothDiceLoss(nn.Module):\n",
        "    # Set momentum = 1 to disable smoothing.\n",
        "    def __init__(self, momentum=0.1):\n",
        "        super(SmoothDiceLoss, self).__init__()\n",
        "        self.momentum = momentum\n",
        "        self.running_denom = -1\n",
        "        self.eps = 1e-5\n",
        "        \n",
        "    def forward(self, score, gt_mask):\n",
        "        score = score.view(score.shape[0], -1)\n",
        "        gt_mask = gt_mask.float().view(gt_mask.shape[0], -1)        \n",
        "        intersect = torch.sum(score * gt_mask, dim=1)\n",
        "        y_sum = torch.sum(gt_mask * gt_mask, dim=1)\n",
        "        z_sum = torch.sum(score * score, dim=1)\n",
        "        denom = z_sum + y_sum + self.eps\n",
        "        mean_denom = denom.mean()\n",
        "        \n",
        "        if self.running_denom == -1:\n",
        "            self.running_denom = mean_denom.item()\n",
        "            dyn_offset = torch.zeros(1, device='cuda')\n",
        "        else:\n",
        "            # Update the running average of the denominator.\n",
        "            self.running_denom = self.running_denom * (1 - self.momentum) \\\n",
        "                                 + mean_denom.item() * self.momentum\n",
        "            # dyn_offset is a tensor of length nBatch.\n",
        "            dyn_offset = self.running_denom - denom.data\n",
        "        # Make denom + dyn_offset = self.running_denom.\n",
        "        smooth_dice = (2 * intersect + self.eps + dyn_offset) / (denom + dyn_offset)\n",
        "        smooth_loss = 1 - smooth_dice\n",
        "        # odice: original dice. oloss: original loss.\n",
        "        orig_dice = (2 * intersect + self.eps) / denom\n",
        "        orig_loss = 1 - orig_dice\n",
        "        \n",
        "        smooth_loss = smooth_loss.mean()\n",
        "        orig_loss   = orig_loss.mean()\n",
        "        #print(\"r-denom: %.1f, denom: %s, offset: %s\" % \\\n",
        "        #        (self.running_denom, str(denom.data.cpu().numpy()), str(dyn_offset.data.cpu().numpy())))\n",
        "        return smooth_loss, orig_loss\n",
        "\n",
        "# Compute each example's dice loss in a batch, and average them                \n",
        "def dice_loss_indiv(score, gt_mask, weight=None):\n",
        "    score = score.view(score.shape[0], -1)\n",
        "    gt_mask = gt_mask.float().view(gt_mask.shape[0], -1)\n",
        "    smooth = 1e-5\n",
        "    intersect = torch.sum(score * gt_mask, dim=1)\n",
        "    y_sum = torch.sum(gt_mask * gt_mask, dim=1)\n",
        "    z_sum = torch.sum(score * score, dim=1)\n",
        "    dice = (2 * intersect + smooth) / (z_sum + y_sum + smooth)\n",
        "    loss = 1 - dice\n",
        "    if weight is not None:\n",
        "        loss = (loss * weight).mean()\n",
        "    else:\n",
        "        loss = loss.mean()\n",
        "    return loss\n",
        "\n",
        "# Treat the whole batch as one big example, and compute the dice loss.\n",
        "def dice_loss_mix(score, gt_mask):\n",
        "    gt_mask = gt_mask.float()\n",
        "    smooth = 1e-5\n",
        "    intersect = torch.sum(score * gt_mask)\n",
        "    y_sum = torch.sum(gt_mask)\n",
        "    z_sum = torch.sum(score)\n",
        "    loss = (2 * intersect + smooth) / (z_sum + y_sum + smooth)\n",
        "    loss = 1 - loss\n",
        "    return loss\n",
        "\n",
        "# vcdr: vertical cup/disc ratio (non-differentiable).\n",
        "# mask_nhot_soft: [B, C, H, W]. \n",
        "# mask_nhot_soft can also be a hard (groundtruth) mask, as threadholding doesn't change it.\n",
        "\n",
        "                    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhpY8DpHq7yz"
      },
      "source": [
        "**OuerNewModel:** **bold text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hi2IPgfLq_45",
        "cellView": "code"
      },
      "source": [
        "#@title OuerNewModel\n",
        "class DoubleConvdown(nn.Module):\n",
        "    \"\"\"(convolution => [IN] => leakyReLU) * 2\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.down_scale=nn.Sequential(nn.Conv3d(in_channels,out_channels,kernel_size=3, stride=1),\n",
        "\t\t\t    nn.InstanceNorm3d(3),\n",
        "\t\t\t    nn.LeakyReLU(),\n",
        "        \tnn.Conv3d(out_channels,out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "\t\t\t    nn.InstanceNorm3d(3),\n",
        "          nn.LeakyReLU())\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.down_scale(x)\n",
        "class OuerNewModel(nn.Module):\n",
        "    \n",
        "    def __init__(self, my_pretrained_model,sets):\n",
        "      \n",
        "        \n",
        "        super(OuerNewModel, self).__init__()\n",
        "        self.my_new_layers = nn.Sequential(nn.Conv3d(4, 3, 3, stride=2),\n",
        "                                           nn.ReLU(),nn.Conv3d(3, 2, 3, stride=2),\n",
        "                                           nn.Conv3d(2, 1, 3, stride=1),\n",
        "                                           nn.ReLU())\n",
        "        self.pretrained = my_pretrained_model\n",
        "        \n",
        "\n",
        "        \n",
        "        self.pool3d = nn.MaxPool3d(kernel_size=2, stride=2)\n",
        "        \n",
        "        # up the sys from ConvTranspose3d to half the channels and do 2 conv3d one from in_ch to out_ch and one out_ch to out_ch\n",
        "        self.up_scale1=nn.Sequential(\n",
        "\t\t\t    nn.ConvTranspose3d(4*sets.n_seg_classes,2*sets.n_seg_classes,kernel_size=2, stride=2),\n",
        "\t\t\t    # should be feat_in*2 or feat_in\n",
        "\t\t\t    nn.Conv3d(2*sets.n_seg_classes, sets.n_seg_classes, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "\t\t\t    nn.InstanceNorm3d(3),\n",
        "\t\t\t    nn.LeakyReLU(),\n",
        "        \tnn.Conv3d(sets.n_seg_classes, sets.n_seg_classes, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "\t\t\t    nn.InstanceNorm3d(3),\n",
        "          nn.LeakyReLU())\n",
        "        #self.up_scale2=norm_lrelu_upscale_conv_norm_lrelu()\n",
        "        \n",
        "        self.up_scale2=nn.Sequential(\n",
        "\t\t\t    nn.ConvTranspose3d(sets.n_seg_classes,sets.n_seg_classes,kernel_size=3, stride=2),\n",
        "\t\t\t    # should be feat_in*2 or feat_in\n",
        "\t\t\t    nn.Conv3d(sets.n_seg_classes, sets.n_seg_classes, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "\t\t\t    nn.InstanceNorm3d(sets.n_seg_classes),\n",
        "\t\t\t    nn.LeakyReLU(),\n",
        "          nn.Conv3d(sets.n_seg_classes, sets.n_seg_classes, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "\t\t\t    nn.InstanceNorm3d(3),\n",
        "          nn.LeakyReLU())\n",
        "      \n",
        "        self.after_cat = nn.Sequential( nn.ConvTranspose3d(sets.n_seg_classes*2,sets.n_seg_classes,kernel_size=1, stride=1),  nn.Conv3d(sets.n_seg_classes, sets.n_seg_classes, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "\t\t\t    nn.InstanceNorm3d(sets.n_seg_classes),\n",
        "\t\t\t    nn.LeakyReLU(),\n",
        "          nn.Conv3d(sets.n_seg_classes, sets.n_seg_classes, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "\t\t\t    nn.InstanceNorm3d(3),\n",
        "          nn.LeakyReLU(),\n",
        "          nn.Conv3d(sets.n_seg_classes, sets.n_seg_classes, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "\t\t\t    nn.InstanceNorm3d(3),\n",
        "          nn.LeakyReLU())\n",
        "        \n",
        "        self.up_scale3=nn.Sequential(\n",
        "\t\t\t    nn.ConvTranspose3d(8*sets.n_seg_classes,4*sets.n_seg_classes,kernel_size=2, stride=2),\n",
        "\t\t\t    # should be feat_in*2 or feat_in\n",
        "\t\t\t    nn.Conv3d(4*sets.n_seg_classes, 2*sets.n_seg_classes, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "\t\t\t    nn.InstanceNorm3d(3),\n",
        "\t\t\t    nn.LeakyReLU(),\n",
        "        \tnn.Conv3d(2*sets.n_seg_classes, 2*sets.n_seg_classes, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "\t\t\t    nn.InstanceNorm3d(3),\n",
        "          nn.LeakyReLU())\n",
        "          \n",
        "        \n",
        "       \n",
        "        self.MaxPool3d =  nn.MaxPool3d(kernel_size=2, stride=2)\n",
        "        self.down1 =  DoubleConvdown(sets.n_seg_classes, 2*sets.n_seg_classes)\n",
        "        self.down2 =  DoubleConvdown(2*sets.n_seg_classes, 4*sets.n_seg_classes)\n",
        "        self.down3 = DoubleConvdown(4*sets.n_seg_classes, 8*sets.n_seg_classes)\n",
        "   \n",
        "    def forward(self, x):\n",
        "        print('x.shape before entry to my new layers:{} '.format(x.shape))\n",
        "        print('x1_img1.shape before entry to my new layers:{} '.format(x[:,0,:,:,:].unsqueeze(1).shape))\n",
        "        x1_img1 = self.pretrained(x[:,0,:,:,:].unsqueeze(1))\n",
        "        print('x1_img1.shape in fowerd pass of till entry to pretrained resnet:{} '.format(x1_img1.shape))\n",
        "        x1_img2 = self.pretrained(x[:,1,:,:,:].unsqueeze(1))\n",
        "        print('x1_img2.shape in fowerd pass of till entry to pretrained resnet:{} '.format(x1_img2.shape))\n",
        "        x1_img3 = self.pretrained(x[:,2,:,:,:].unsqueeze(1))\n",
        "        print('x1_img3.shape in fowerd pass of till entry to pretrained resnet:{} '.format(x1_img3.shape))\n",
        "        x1_img4 = self.pretrained(x[:,3,:,:,:].unsqueeze(1))\n",
        "        print('x1_img4.shape in fowerd pass of till entry to pretrained resnet:{} '.format(x1_img4.shape))\n",
        "        \n",
        "        x2 =torch.cat((x1_img1, x1_img2), dim=1)\n",
        "        x2 = torch.cat((x2, x1_img3), dim=1)\n",
        "        x2 = torch.cat((x2, x1_img4), dim=1)\n",
        "        print('x2.shape in fowerd pass after passing the 4 images throw the pretrained and concatnted: {}'.format(x2.shape))\n",
        "        x2_conv = self.down3(x2)\n",
        "        \n",
        "        print('x2_conv.shape in fowerd pass after passing the 4 images throw the pretrained and concatnted: {}'.format(x2_conv.shape))\n",
        "\n",
        "\n",
        "        x_downsized1 =  self.down1(x)\n",
        "        x_downsized1 = self.MaxPool3d(x_downsized1)\n",
        "        print('x_downsized1.shape : {}'.format(x_downsized1.shape))\n",
        "\n",
        "        x_downsized2 =  self.down2(x_downsized1)\n",
        "        x_downsized2 = self.MaxPool3d(x_downsized2)\n",
        "       \n",
        "        print('x_downsized2.shape : {}'.format(x_downsized2.shape))\n",
        "        \n",
        "        x_downsized3 =  self.down3(x_downsized2)\n",
        "        \n",
        "        print('x_downsized3.shape : {}'.format(x_downsized3.shape))\n",
        "        \n",
        "        #concatinting x_downsized2 to x2:\n",
        "        x_downsized2_reshaped = torch.nn.functional.interpolate(x_downsized2,x2.shape[2:], scale_factor=None, mode='nearest')\n",
        "        print('x_downsized2_reshaped.shape : {}'.format(x_downsized2_reshaped.shape))\n",
        "        x_downsized2_concatinted = torch.cat((x_downsized2_reshaped,x2),dim=1)\n",
        "        print('x_downsized2_concatinted.shape : {}'.format(x_downsized2_concatinted.shape))\n",
        "        x_downsized2_concatinted_up3 = self.up_scale3(x_downsized2_concatinted)\n",
        "        print('x_downsized2_concatinted_up3.shape : {}'.format(x_downsized2_concatinted_up3.shape))\n",
        "\n",
        "        #concatinting x_downsized1 to x_downsized2_concatinted_up3:\n",
        "        x_downsized1_reshaped = torch.nn.functional.interpolate(x_downsized1,x_downsized2_concatinted_up3.shape[2:], scale_factor=None, mode='nearest')\n",
        "        print('x_downsized1_reshaped.shape : {}'.format(x_downsized1_reshaped.shape))\n",
        "        x_downsized1_concatinted = torch.cat((x_downsized1_reshaped,x_downsized2_concatinted_up3),dim=1)\n",
        "        print('x_downsized1_concatinted.shape : {}'.format(x_downsized1_concatinted.shape))\n",
        "        x_downsized1_concatinted_up1 = self.up_scale1(x_downsized1_concatinted)\n",
        "        print('x_downsized1_concatinted_up1.shape : {}'.format(x_downsized1_concatinted_up1.shape))\n",
        "\n",
        "\n",
        "        #concatinting x to x_downsized1_concatinted_up1:\n",
        "        x_downsized1_concatinted_up1_reshaped = torch.nn.functional.interpolate(x_downsized1_concatinted_up1,x.shape[2:], scale_factor=None, mode='nearest')\n",
        "        print('x_downsized1_concatinted_up1_reshaped.shape : {}'.format(x_downsized1_concatinted_up1_reshaped.shape))\n",
        "        x_reshaped_concatinted = torch.cat((x,x_downsized1_concatinted_up1_reshaped),dim=1)\n",
        "        print('x_reshaped_concatinted.shape : {}'.format(x_reshaped_concatinted.shape))\n",
        "        x_reshaped_concatinted_up1 = self.after_cat(x_reshaped_concatinted)\n",
        "        print('x_reshaped_concatinted_up1 ,aftercat.shape after the last conv  : {}'.format(x_reshaped_concatinted_up1.shape))\n",
        "        #x_out_reshped_to_x =torch.nn.functional.interpolate(x_reshaped_concatinted_up1,x.shape[2:], scale_factor=None, mode='nearest')\n",
        "        #print('x_out_reshped_to_x .shape  : {}'.format(x_out_reshped_to_x.shape))\n",
        "\n",
        "\n",
        "       \n",
        "        return x_reshaped_concatinted_up1\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "def norm_lrelu_upscale_conv_norm_lrelu(self, size):\n",
        "        \n",
        "\t\t    return nn.Sequential(\n",
        "\t\t\t    nn.InstanceNorm3d(feat_in),\n",
        "\t\t\t    nn.LeakyReLU(),\n",
        "\t\t\t    nn.Upsample(size, mode='nearest'),\n",
        "\t\t\t    # should be feat_in*2 or feat_in\n",
        "\t\t\t    nn.Conv3d(feat_in, feat_out, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "\t\t\t    nn.InstanceNorm3d(feat_out),\n",
        "\t\t\t    nn.LeakyReLU())\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ha5ySHprLJ_S"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qp6dSDlf3Cn1"
      },
      "source": [
        "**test the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXZD1COc3FuA"
      },
      "source": [
        "#model, parameters = generate_model(sets) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHAygT94-SAs"
      },
      "source": [
        "**add Utils**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "EkuHCXtg-U4s"
      },
      "source": [
        "#@title load_lines\n",
        "\n",
        "\n",
        "\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(\n",
        "    format='%(asctime)s %(levelname)-8s [%(filename)s:%(lineno)d] %(message)s',\n",
        "    datefmt='%Y-%m-%d %H:%M:%S',\n",
        "    level=logging.DEBUG)\n",
        "\n",
        "log = logging.getLogger()\n",
        "\n",
        "import os\n",
        "import os.path as osp\n",
        "\n",
        "def load_lines(file_path):\n",
        "    \"\"\"Read file into a list of lines.\n",
        "    Input\n",
        "      file_path: file path\n",
        "    Output\n",
        "      lines: an array of lines\n",
        "    \"\"\"\n",
        "    with open(file_path, 'r') as fio:\n",
        "        lines = fio.read().splitlines()\n",
        "    return lines"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0auvWe48lZz"
      },
      "source": [
        "**check the train code**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "bDllVi_R8rRe"
      },
      "source": [
        "#@title  train code\n",
        "from torch import nn\n",
        "import itertools\n",
        "from torch import optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "\n",
        "from scipy import ndimage\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "\n",
        "def train(data_loader, model, optimizer, scheduler, total_epochs, save_interval, save_folder, sets):\n",
        "    # settings\n",
        "    batches_per_epoch = len(data_loader)\n",
        "    log.info('{} epochs in total, {} batches per epoch'.format(total_epochs, batches_per_epoch))\n",
        "    \n",
        "    loss_seg = nn.CrossEntropyLoss()\n",
        "    bce_loss_func = nn.BCEWithLogitsLoss( pos_weight=sets.bce_weight.cuda())\n",
        "    \n",
        "    counter=0\n",
        "    best_loss=1\n",
        "    losses=[]\n",
        "\n",
        "   \n",
        "\n",
        "    print(\"Current setting is:\")\n",
        "    print(sets)\n",
        "    print(\"\\n\\n\")     \n",
        "    if not sets.no_cuda:\n",
        "        loss_seg = loss_seg.cuda()\n",
        "        \n",
        "    model.train()\n",
        "    train_time_sp = time.time()\n",
        "\n",
        "    class_weights = torch.ones(sets.n_seg_classes).cuda()\n",
        "    class_weights[0] = 0\n",
        "    \n",
        "    class_weights /= class_weights.sum()\n",
        "    \n",
        "    for epoch in range(total_epochs):\n",
        "        log.info('Start epoch {}'.format(epoch))\n",
        "        \n",
        "        scheduler.step()\n",
        "\n",
        "        #------moved the schduler to be after the optimizer------\n",
        "\n",
        "\n",
        "        log.info('lr = {}'.format(scheduler.get_lr()))\n",
        "        print('len data_loader = {} '.format(len(data_loader)))\n",
        "        for batch_id, batch_data in enumerate(data_loader):\n",
        "            dice_losses = []\n",
        "            total_dice_loss = 0\n",
        "            \n",
        "            \n",
        "            \n",
        "            # getting data batch\n",
        "            counter+=1\n",
        "            batch_id_sp = epoch * batches_per_epoch\n",
        "           \n",
        "            if not sets.no_cuda: \n",
        "                volumes, mask = batch_data['image'].cuda(), batch_data['mask'].cuda()\n",
        "                print('cuda')\n",
        "            else:\n",
        "              volumes, mask = batch_data['image'], batch_data['mask']\n",
        "\n",
        "            print('volumes.shape ={}'.format(volumes.shape ))\n",
        "            print('mask.shape ={}'.format(mask.shape ))\n",
        "            mask = brats_map_label(mask, sets.n_seg_classes)\n",
        "            print('mask.shape after brats map label={}'.format(mask.shape ))\n",
        "            mask = torch.tensor(mask)\n",
        "            mask = mask.permute(1, 0, 2,3,4)\n",
        "            print('mask.shape after premute ={}'.format(mask.shape ))\n",
        "            \n",
        "\n",
        "          \n",
        "           \n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            out_masks = model(volumes)\n",
        "            print('out_masks.shape ={}'.format(out_masks.shape ))\n",
        "            \n",
        "\n",
        "            !CUDA_LAUNCH_BLOCKING=1\n",
        "            label_masks= mask\n",
        "\n",
        "            DICE_W =0.2\n",
        "\n",
        "           \n",
        "            outputs_soft    = torch.sigmoid(out_masks)\n",
        "            \n",
        "            print('outputs_soft.shape ={}'.format(outputs_soft.shape ))\n",
        "            \n",
        "            \n",
        "             \n",
        "\n",
        "           \n",
        "\n",
        "            print('out_masks.shape before ce loss ={}'.format(out_masks.shape ))\n",
        "            print('mask.shape before ce loss={}'.format(mask.shape ))\n",
        "            print('out_masks.shape before ce loss ={}'.format(out_masks.shape ))\n",
        "            print('mask.shape before ce loss={}'.format(mask.shape ))\n",
        "            #----- trying omthing new ----\n",
        "            total_ce_loss   = bce_loss_func(out_masks.permute([0,2,3,4,1]), \n",
        "                                            # after brats_map_label(), dim 1 of mask_batch is segmantation class.\n",
        "                                            # It's permuted to the last dim to align with outputs for bce loss computation.\n",
        "                                            mask.permute([0,2,3,4,1]))\n",
        "\n",
        "\n",
        "            \n",
        "            \n",
        "\n",
        "            \n",
        "            ### calculaiting the dice loss for evry chanel\n",
        "            for cls in range(1, sets.n_seg_classes):\n",
        "                allcls_pred_np  = outputs_soft.data.cpu().numpy()\n",
        "                allcls_gt_np    = mask.data.cpu().numpy()\n",
        "                pred = allcls_pred_np[:, cls].astype(np.uint8)\n",
        "                gt   = allcls_gt_np[ :, cls].astype(np.uint8)\n",
        "                dice_loss = 1- metric.binary.dc(gt, pred)\n",
        "                dice_losses.append(dice_loss)\n",
        "                total_dice_loss = total_dice_loss + dice_loss * class_weights[cls]\n",
        "            \n",
        "            # calculating loss\n",
        "            \n",
        "            loss = (1 - DICE_W) * total_ce_loss + DICE_W * total_dice_loss\n",
        "            print('total_ce_loss={}'.format(total_ce_loss))\n",
        "            print('total_dice_loss={}'.format(total_dice_loss))\n",
        "            losses.append(loss)\n",
        "            \n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            \n",
        "            \n",
        "            optimizer.step()\n",
        "\n",
        "            avg_batch_time = (time.time() - train_time_sp) / (1 + batch_id_sp)\n",
        "            log.info(\n",
        "                    'Batch: {}-{} ({}), loss = {:.3f}, loss_seg = {:.3f}, avg_batch_time = {:.3f}'\\\n",
        "                    .format(epoch, batch_id, batch_id_sp, loss.item(), loss.item(), avg_batch_time))\n",
        "          \n",
        "            if not sets.ci_test:\n",
        "                # save model\n",
        "                if  counter != 0 and counter % save_interval == 0 :\n",
        "                \n",
        "                    model_save_path = '{}_epoch_{}_batch_{}.pth.tar'.format(save_folder, epoch, batch_id)\n",
        "                    best_loss = loss\n",
        "                    model_save_dir = os.path.dirname(model_save_path)\n",
        "                    if not os.path.exists(model_save_dir):\n",
        "                        os.makedirs(model_save_dir)\n",
        "                    \n",
        "                    log.info('Save checkpoints: epoch = {}, batch_id = {}'.format(epoch, batch_id)) \n",
        "                    print('saved the model in {}'.format(model_save_path))\n",
        "                    sets.resume_path= model_save_path\n",
        "                    torch.save({\n",
        "                                'ecpoch': epoch,\n",
        "                                'batch_id': batch_id,\n",
        "                                'state_dict': model.state_dict(),\n",
        "                                'optimizer': optimizer.state_dict()},\n",
        "                                model_save_path)\n",
        "                    #print('saved the model')\n",
        "                            \n",
        "    print('Finished training')\n",
        "    xaxis= list(range(0, counter))\n",
        "    plt.plot(xaxis,losses)\n",
        "    plt.title(' Loss ')\n",
        "\n",
        "    # naming the x axis\n",
        "    plt.xlabel('Pa')\n",
        "    # naming the y axis\n",
        "    plt.ylabel('Loss')\n",
        "\n",
        "    # function to show the plot\n",
        "    plt.show()            \n",
        "    if sets.ci_test:\n",
        "        exit()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "      \n",
        "      sets =parse_opts()  \n",
        "\n",
        "      print('passed sets')\n",
        "\n",
        "\n",
        "      sets.img_list = path/'train.txt'\n",
        "      sets.no_cuda = False\n",
        "      sets.gpu_id = [0]\n",
        "      sets.data_root = path \n",
        "      sets.pretrain_path =  path/'pretrain/resnet_10.pth'\n",
        "      sets.num_workers = 0\n",
        "      sets.n_seg_classes = 4\n",
        "      sets.model_depth = 10\n",
        "      sets.resnet_shortcut = 'B'\n",
        "      sets.ci_test = False\n",
        "      sets.save_intervals=10\n",
        "      sets.n_epochs=10\n",
        "      sets.batch_size = 1\n",
        "     \n",
        "      sets.input_D = 120 #14\n",
        "      sets.input_H = 130 #28\n",
        "      sets.input_W = 130 #28\n",
        "      sets.orig_patch_size= '152,160,152'\n",
        "\n",
        "      #-- added somthing for the test, this is taken from https://github.com/askerlee/segtran/blob/master/code/train3d.py train code\n",
        "      sets.bce_weight =[0., 3, 1, 1.75]  # bg, ET, WT, TC\n",
        "\n",
        "      #sets.bce_weight = torch.tensor(sets.bce_weight).cuda()\n",
        "      sets.bce_weight = torch.tensor(sets.bce_weight).cpu()\n",
        "      sets.bce_weight = sets.bce_weight * (sets.n_seg_classes - 1) / sets.bce_weight.sum()\n",
        "\n",
        "\n",
        "      \n",
        "      sets.save_folder = \"/content/drive/My Drive/Deep Learning Final Project/trails/models/{}_{}\".format( sets.model,  sets.model_depth )\n",
        "    \n",
        "      \n",
        " \n",
        "     \n",
        "    \n",
        "      torch.manual_seed(sets.manual_seed)\n",
        "      class Identity(nn.Module):\n",
        "          def __init__(self):\n",
        "            super(Identity, self).__init__()\n",
        "\n",
        "          def forward(self, x):\n",
        "              return x\n",
        "      \n",
        "      model, parameters = generate_model(sets)\n",
        "      model.conv_seg = Identity()\n",
        "      \n",
        "      #for param in model.parameters():\n",
        "      #    param.requires_grad = False\n",
        "      #for name, param in model.named_parameters():\n",
        "      #    print('name = {}'.format(name))\n",
        "      #    param.requires_grad = False\n",
        "      #    if name == 'module.conv_seg.6.weight':\n",
        "      #      param.requires_grad = True\n",
        "      #      print('{} with  grad'.format(name))\n",
        "\n",
        "      \n",
        "      \n",
        "      model=OuerNewModel(model,sets)\n",
        "      #print (model)\n",
        "      if torch.cuda.is_available():\n",
        "        model.cuda()\n",
        "      \n",
        "      # optimizer\n",
        "      if sets.ci_test:\n",
        "          params = [{'params': parameters, 'lr': sets.learning_rate}]\n",
        "      else:\n",
        "          params = [\n",
        "                  { 'params': parameters['base_parameters'], 'lr': sets.learning_rate }, \n",
        "                  { 'params': parameters['new_parameters'], 'lr': sets.learning_rate*100 }\n",
        "                  ]\n",
        "      \n",
        "      optimizer = torch.optim.Adam(model.parameters(),lr=sets.learning_rate)#learning_rate)\n",
        "          \n",
        "      scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
        "      \n",
        "\n",
        "      # train from resume\n",
        "      if sets.resume_path:\n",
        "          \n",
        "          if os.path.isfile(sets.resume_path):\n",
        "              print('resumed the path')\n",
        "              print(\"=> loading checkpoint '{}'\".format(sets.resume_path))\n",
        "              checkpoint = torch.load(sets.resume_path)\n",
        "              model.load_state_dict(checkpoint['state_dict'])\n",
        "              optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "              print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
        "                .format(sets.resume_path, checkpoint['epoch']))\n",
        "              print('loaded the model')\n",
        "              \n",
        "\n",
        "      # getting data\n",
        "      sets.phase = 'train'\n",
        "      if sets.no_cuda:\n",
        "          sets.pin_memory = False\n",
        "      else:\n",
        "          sets.pin_memory = True    \n",
        "      \n",
        "      training_dataset = BratsSet(sets,sets.data_root, sets.img_list)\n",
        "      data_loader = DataLoader(training_dataset, batch_size=sets.batch_size, shuffle=True, num_workers=sets.num_workers, pin_memory=sets.pin_memory)\n",
        "      \n",
        "\n",
        "      \n",
        "##############################################\n",
        "      print(sets.save_folder)\n",
        "      # training\n",
        "      train(data_loader, model, optimizer, scheduler, total_epochs=sets.n_epochs, save_interval=sets.save_intervals, save_folder=sets.save_folder, sets=sets) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SnbElDz6Qdi"
      },
      "source": [
        "vizualize the network"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5724DsZ0g4RB"
      },
      "source": [
        "**add the tset func**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aXJbnltey4n"
      },
      "source": [
        "      dataset_iter = iter(data_loader)\n",
        "      batch_data = next(dataset_iter)\n",
        "      #print('data_loader = {}'.format(data_loader))\n",
        "      if not sets.no_cuda: \n",
        "          volumes, mask = batch_data['image'].cuda(), batch_data['mask'].cuda()\n",
        "          print('cuda')\n",
        "      else:\n",
        "        volumes, mask = batch_data['image'], batch_data['mask']\n",
        "      \n",
        "      print('volumes.shape before fowered pass ={}'.format(volumes.shape ))\n",
        "      print('mask.shape before fowered pass ={}'.format(mask.shape ))\n",
        "      mask = brats_map_label(mask, sets.n_seg_classes)\n",
        "      print('mask.shape after brats map label={}'.format(mask.shape ))\n",
        "      mask = torch.tensor(mask)\n",
        "      mask = mask.permute(1, 0, 2,3,4)\n",
        "      print('mask.shape after premute ={}'.format(mask.shape ))\n",
        "      optimizer.zero_grad()\n",
        "      out_masks = model(volumes)\n",
        "      print('out_masks.shape ={}'.format(out_masks.shape ))\n",
        "\n",
        "      outputs_soft    = torch.sigmoid(out_masks)\n",
        "     \n",
        "      outputs_soft = torch.where(outputs_soft>0.5, 1, 0) \n",
        "      out_masks = outputs_soft\n",
        "      print('outputs_soft.shape ={}'.format(outputs_soft.shape ))\n",
        "      print('vizualize volumes and label AFTER fowerd pass channel 0:')\n",
        "      vizualize_a_single_img_and_its_lable(out_masks[0,0].cpu().detach().numpy(), mask[0,0].cpu(), montage_display = True)\n",
        "      print('vizualize volumes and label AFTER fowerd pass channel 1:')\n",
        "      vizualize_a_single_img_and_its_lable(out_masks[0,1].cpu().detach().numpy(), mask[0,1].cpu(), montage_display = True)\n",
        "      print('vizualize volumes and label AFTER fowerd pass channel 2:')\n",
        "      vizualize_a_single_img_and_its_lable(out_masks[0,2].cpu().detach().numpy(), mask[0,2].cpu(), montage_display = True)\n",
        "      print('vizualize volumes and label AFTER fowerd pass channel 3:')\n",
        "      vizualize_a_single_img_and_its_lable(out_masks[0,3].cpu().detach().numpy(), mask[0,3].cpu(), montage_display = True)\n",
        "   \n",
        "   \n",
        "   \n",
        "         "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "rFnFlYGWg9Q1"
      },
      "source": [
        "#@title  test code\n",
        "\n",
        "def test_calculate_metric(sets,iter_nums,net,db_test):\n",
        "   \n",
        "   \n",
        "    loss1_WT=0\n",
        "    loss1_ET=0\n",
        "    loss1_TC=0\n",
        "    loss2_WT=0\n",
        "    loss2_ET=0\n",
        "    loss2_TC=0\n",
        " \n",
        "    counter=0\n",
        "    net.cuda()\n",
        "    net.eval()\n",
        "    preproc_fn = None\n",
        "    test_save_paths=[None]\n",
        "    has_mask =    False\n",
        "\n",
        "    for batch_id, batch_data in enumerate(data_loader):\n",
        "\n",
        "    \n",
        "        \n",
        "      if not sets.no_cuda: \n",
        "                volumes, mask = batch_data['image'].cuda(), batch_data['mask'].cuda()\n",
        "                print('cuda')\n",
        "      else:\n",
        "              volumes, mask = batch_data['image'], batch_data['mask']\n",
        "      counter+=1 \n",
        "      print('volumes.shape ={}'.format(volumes.shape ))\n",
        "      print('mask.shape ={}'.format(mask.shape ))\n",
        "      mask = brats_map_label(mask, sets.n_seg_classes)\n",
        "      print('mask.shape after brats map label={}'.format(mask.shape ))\n",
        "      mask = torch.tensor(mask)\n",
        "      mask = mask.permute(1, 0, 2,3,4)\n",
        "      print('mask.shape after premute ={}'.format(mask.shape )) \n",
        "      out_masks = model(volumes)\n",
        "      print('out_masks.shape ={}'.format(out_masks.shape ))\n",
        "      outputs_soft    = torch.sigmoid(out_masks)\n",
        "      outputs_soft = torch.where(outputs_soft>0.5, 1, 0) \n",
        "      print('outputs_soft.shape ={}'.format(outputs_soft.shape ))\n",
        "      dice_losses1 = []\n",
        "      dice_losses2 = []\n",
        "      total_dice_loss1 = 0\n",
        "      total_dice_loss2 = 0\n",
        "      for cls in range(1, sets.n_seg_classes):\n",
        "                if counter!=0 and counter% sets.save_intervals ==0:\n",
        "                  print('vizualize volumes and label AFTER fowerd pass channel {}:'.format(cls))\n",
        "                  vizualize_a_single_img_and_its_lable(outputs_soft[0, cls].cpu().detach().numpy(), mask[0, cls].cpu().detach().numpy(), montage_display = True)\n",
        "                  \n",
        "                 \n",
        "      loss1_WT= (loss1_WT*(counter-1)+dice_loss_indiv(outputs_soft[:, 1], mask[ :, 1]))/counter\n",
        "      loss1_ET=(loss1_ET*(counter-1)+dice_loss_indiv(outputs_soft[:, 2], mask[ :, 2]))/counter\n",
        "      loss1_TC=(loss1_ET*(counter-1)+dice_loss_indiv(outputs_soft[:, 3], mask[ :, 3]))/counter\n",
        "      allcls_pred_np  = outputs_soft.data.cpu().numpy()\n",
        "      allcls_gt_np    = mask.data.cpu().numpy()\n",
        "      pred = allcls_pred_np[:, 1].astype(np.uint8)\n",
        "      gt   = allcls_gt_np[ :, 1].astype(np.uint8)\n",
        "      loss2_WT= (loss2_WT*(counter-1)+(1-metric.binary.dc(gt, pred)))/counter\n",
        "      pred = allcls_pred_np[:, 2].astype(np.uint8)\n",
        "      gt   = allcls_gt_np[ :, 2].astype(np.uint8)\n",
        "      loss2_ET= (loss2_ET*(counter-1)+(1-metric.binary.dc(gt, pred)))/counter\n",
        "      pred = allcls_pred_np[:, 3].astype(np.uint8)\n",
        "      gt   = allcls_gt_np[ :, 3].astype(np.uint8)\n",
        "      loss2_TC= (loss2_TC*(counter-1)+(1-metric.binary.dc(gt, pred)))/counter\n",
        "\n",
        "\n",
        "\n",
        "    print('loss1_WT={}'.format(loss1_WT))\n",
        "    print('loss1_TC={}'.format(loss1_TC)) \n",
        "    print('loss1_ET={}'.format(loss1_ET))\n",
        "    print('loss2_WT={}'.format(loss2_WT))\n",
        "    print('loss2_TC={}'.format(loss2_TC)) \n",
        "    print('loss2_ET={}'.format(loss2_ET))          \n",
        "    return loss1_WT,loss1_WT,counter\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # settting\n",
        "      sets = parse_opts()\n",
        "      sets.target_type = \"normal\"\n",
        "      sets.phase = 'train'\n",
        "      sets.resume_path = '/content/drive/My Drive/Deep Learning Final Project/trails/models/resnet_10_epoch_3_batch_159.pth.tar'\n",
        "      sets.pretrain_path =  path/'pretrain/resnet_10.pth'\n",
        "      sets.img_list =   path/'valid.txt'\n",
        "      sets.model_depth = 10\n",
        "      sets.n_seg_classes = 4\n",
        "      sets.save_intervals=10\n",
        "      sets.gpu_id = [0]\n",
        "      sets.orig_patch_size= [136,152,136]\n",
        "      sets.no_cuda = False\n",
        "      sets.iters ='100,500,1000'\n",
        "      sets.input_patch_size='136, 152, 136'\n",
        "      sets.batch_size = 1 \n",
        "\n",
        "      print(\"Current setting is:\")\n",
        "      print(sets)\n",
        "      print(\"\\n\\n\")\n",
        "\n",
        "     \n",
        "      print('sets.resume_path={}'.format(sets.resume_path))\n",
        "      checkpoint = torch.load(sets.resume_path)\n",
        "      net, _ = generate_model(sets)\n",
        "      net = model=OuerNewModel(net,sets)\n",
        "      net.load_state_dict(checkpoint['state_dict'])\n",
        "      print('finished loading the model')\n",
        "\n",
        "      \n",
        "      testing_data = BratsSet(sets,sets.data_root, sets.img_list)\n",
        "      data_loader = DataLoader(testing_data, batch_size=sets.batch_size, shuffle=False, num_workers=1, pin_memory=False)\n",
        "      iter_nums = [ int(i) for i in sets.iters.split(\",\") ]\n",
        "      with torch.no_grad():\n",
        "              print('kupi')\n",
        "              dices1,dices2,counter = test_calculate_metric(sets,iter_nums,net,data_loader)\n",
        "              "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}